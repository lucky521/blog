---
title: "卷积神经网络"
categories: [design]
layout: post
---

卷积神经网络近几年在图像视觉领域的发展使得深度学习成为大热门。


# 卷积神经网络的结构

卷积神经网络是用到了多个一样的神经元，可以表达更大的模型。

我们在写程序的时候，会写一个函数，然后在多个地方调用这个函数。这是函数的复用作用。类似的，卷积神经网络学习一个神经元，然后在多个地方复用它。


假设你有一个神经网络，训练声音样本，预测是否有人声在里面。

最简单的方法是把所有声音样本按时间平分，作为同一层的输入。复杂的方法是加入数据的其他属性，比如频率。

## 卷积操作

所谓卷积神经网络，指的是至少在神经网络的某一层使用卷积运算来替代一般的矩阵乘法运算。

s(t) = (x * w)(t)

比如上面是用函数w对函数x做卷积，原函数w是t的函数x(t)，函数w称作核函数，卷积时候的新函数是s(t)。输入t可以是多维的，比如多维数组(i,j,k,...)。对于二维的情况，卷积对应了一个双重分块循环矩阵。

为什么卷积运算有助于改进神经网络？

    稀疏交互：通过核函数的大小来控制输入影响的范围。
    参数共享：非卷积的运算中参数只会用到一次。而卷积的核参数是多次使用的。
    等变表示：如果原函数是输入的平移函数，那么卷积函数对原函数有等变性。


## 池化操作

池化操作是几乎所有卷积网络都会有的操作。池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。例如，MAX pooling函数是给出相邻矩阵区域内的最大值，当然还有平均值、加权平均值等其他池化函数。

池化的平移不变性指的是当我们对输入进行少量平移时，经过池化函数后的大多数输出不会发生变化。当我们关心一个特征是否出现而不是关心在哪里出现时，局部平移不变性是很有用的性质。

典型的卷积神经网络层有三级。

    第一级卷积层：仿射变换
    第二级探测层：整流线性
    第三级池化层：不变性


## 卷积层 Convolutional Layer

## 全连接层 Fully-Connected Layer

## 扁平层 Flatten layer

Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。

The Flatten layer is a utility layer that flattens an input of shape n * c * h * w to a simple vector output of shape n * (c*h*w).


# 典型的卷积神经网络模型

## LeNet

http://yann.lecun.com/exdb/lenet/

## AlexNet

从2012年这篇CNN的大作之后，卷积神经网络彻底火起来。

http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks

## VggNet

## GoogLeNet,  Inception V3 model

http://arxiv.org/abs/1409.4842

## ResNet

## CaffeNet

## MobileNet

https://arxiv.org/abs/1704.04861





# CNN benchmarks

https://github.com/jcjohnson/cnn-benchmarks
