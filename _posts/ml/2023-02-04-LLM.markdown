---
layout: post
title:  "大模型时代"
subtitle: "Large language model"
categories: [MachineLearning]
---

# 大规模预训练语言模型 （大模型）
Generative Pretrained Transformer ，也有个叫法是大型语言模型（LLM）。
相比于在此之前的NLP模型，它能完成更加通用和智能的NLP任务，比如搜索、推荐、问答、内容创作、写代码。

OpenAI的GPT系列将大模型推向了大风口。
* 2018.06 GPT 1.2亿参数
* 2019.02 GPT-2 15亿参数
* 2020.05 GPT-3 1750亿参数
* 2022.12 ChatGPT
* 2023.03 GPT-4

## 预训练架构
NLP各种任务其实收敛到了两个不同的预训练模型框架里：
* 对于自然语言理解类任务，其技术体系统一到了以Bert为代表的“双向语言模型预训练+应用Fine-tuning”模式；
* 对于自然语言生成类任务，其技术体系统一到了以GPT 2.0为代表的“自回归语言模型（即从左到右单向语言模型）+Zero /Few Shot Prompt”模式

* 自编码（autoencoding，AE）：在输入文本中，随机删除连续的一个或者多个token，然后通过上下文来预测该token。这类模型主要以Bert为代表（Mask Language Model）。
* 自回归（autoregressive，AR）：通常来讲是根据上文内容预测下一个可能的token（实际上反过来也可以，通过下文预测上文的单词），如GPT系列。
对比下，自编码由于抠字，不太适合做NLG（Natural Language Generation）的任务（训练和预测过程不一致），而自回归由于属于只能看到一侧的信息，在做NLU任务上有缺陷（类似ELMo这种双向自回归的看上去能够解这个问题，实际上效果见仁见智），却天然适合NLG。




## LLM模型结构

Transformer 架构已成为开发各种 LLM 的事实标准骨干，现有 LLM 的主流架构可以大致分为三种类型:
* 编码器-解码器架构
* 因果解码器架构
* 前缀解码器架构

Layer Norm
位置编码

## 提示学习 Prompt Learning
比如做情感分类任务：
* 监督学习的做法是输入“我今天考砸了”，模型输出分类的分数或分布.
* 而提示学习的做法则是在“我今天考砸了”后拼接上自然语言描述“我感觉很 ____”，让模型生成后面的内容，再根据某种映射函数，将 生成内容匹配到某一分类标签。


## 指令精调(Instruction Tuning)
让LLM理解输入命令的含义，并正确执行


## 提示工程（Prompt Engineering）
https://www.promptingguide.ai/zh


## 有监督微调（SFT）



## In-flight Batching / continuous batching / iteration-level batching
在处理实时数据流（如聊天消息）时，连续批处理会不断地将即时到达的消息聚合成批次，并将每个批次作为一个单元送入模型进行处理。这种方式可以减少模型推理的延迟并提高吞吐量。
https://www.anyscale.com/blog/continuous-batching-llm-inference

### stream_chat 流式输出
在使用ChatGPT时，模型的回复内容是一个字一个字蹦出来的，而不是整段话直接出现，因为模型需要不断预测接下来要回复什么内容，如果等整段回复生成之后再输出到网页，用户体验就会很差，一直以为这种流式输出效果是用WebSocket实现的，后来接入openai接口，发现接口是http协议，才了解到SSE技术。

Server-Sent Events (SSE) 是一种基于 HTTP 协议的服务器推送技术，它允许服务器向客户端发送数据和信息。与 WebSocket 不同，SSE 是一种单向通信方式，只有服务器可以向客户端推送消息。SSE 是 HTML5 规范的一部分，使用非常简单，主要由服务端与浏览器端的通讯协议（HTTP协议）和 EventSource 接口来处理 Server-sent events 组成，服务器端的响应的内容类型是“text/event-stream”.







# 开源大模型
* llama
  * 推理：
    * https://github.com/ggerganov/llama.cpp
* falcon
* chatGLM 
  * 教学：
    * https://keg.cs.tsinghua.edu.cn/jietang/publications/ChatGLM&Beyond.pdf
    * https://www.bilibili.com/video/BV1x34y1A7uQ
  * 部署和微调：
    * https://huggingface.co/THUDM/chatglm2-6b
    * https://github.com/THUDM/ChatGLM2-6B
  * chatGLM 模型结构
    * PrefixEncoder
    * GLMBlock
      * SelfAttention
    * RotaryEmbedding 位置编码
    * GLMTransformer
    * RMSNorm
* baichuan
* qwen
* bloom
  * https://huggingface.co/bigscience/bloom




# 大模型微调技术
https://zhuanlan.zhihu.com/p/618894319
* 2019年 Houlsby N 等人提出的 Adapter Tuning
* 2021年微软提出的 LORA
* 斯坦福提出的 Prefix-Tuning
* 谷歌提出的 Prompt Tuning
* 2022年清华提出的 P-tuning v2

huggingface PEFT
把微调技术工程化了 https://huggingface.co/docs/peft/index






# LLM大模型部署
推荐阅读[LLM 的推理优化技术纵览](https://zhuanlan.zhihu.com/p/642412124)

## fastllm
https://github.com/ztxz16/fastllm

https://zhuanlan.zhihu.com/p/646193833

## FlashAttention
https://github.com/Dao-AILab/flash-attention

## vllm
vLLM 主要用于快速 LLM 推理和服务，其核心是 PagedAttention，这是一种新颖的注意力算法
https://github.com/vllm-project/vllm

## chatglm.cpp
https://github.com/li-plus/chatglm.cpp

## llama.cpp
https://github.com/ggerganov/llama.cpp

## whisper.cpp
https://github.com/ggerganov/whisper.cpp
https://github.com/openai/whisper

## text-generation-inference
https://github.com/huggingface/text-generation-inference

## lmdeploy
https://github.com/InternLM/lmdeploy




## openAI 服务接口
你可参考openai官网或者https://openai.apifox.cn/

普通问答接口
```shell
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
     "model": "gpt-4-turbo",
     "messages": [{"role": "user", "content": "Say this is a test!"}],
     "temperature": 0.7
   }'
```

查询Embedding接口
```shell
curl https://api.openai.com/v1/embeddings \
 -H "Content-Type: application/json" \
 -H "Authorization: Bearer $OPENAI_API_KEY" \
 -d '{
  "input": "Your text string goes here",
  "model": "text-embedding-ada-002"
}'
```

模型微调接口

```shell
curl https://api.openai.com/v1/fine_tuning/jobs \
 -H "Content-Type: application/json" \
 -H "Authorization: Bearer $OPENAI_API_KEY" \
 -d '{
    "training_file": "file-prompt-completion.jsonl",
    "model": "gpt-3.5-turbo"
}'
```


# 多模态

图像、声音、视频

## DALL-E









# 关注大模型的落地应用
如果openai把GPT做到像OS一样，那它也许就会如windows操作系统一样垄断，并且你没有机会也没有必要去自己开发GPT，绝大多数人的归属就是基于它做应用即可。

## 知识问答

## 文字创作

## 写代码

## 智能控制
GPT强大的理解能力，能够帮人类完成一些事情的感知->理解->决策->响应。 
让GPT读、写任何具备API接口能力的服务、设备。


# AI Agent
Agent代理的意思，显而易见，个人助理。Agent并非ChatGPT升级版，它不仅告诉你“如何做”，更会帮你去做.

Agent = LLM+Planning+Feedback+Tool use


# RAG 检索增强生成
LLM 擅长于一般的语言理解与推理，而不是某个具体的知识点。如何为ChatGPT/LLM大语言模型添加额外知识？

当用户提出问题时，系统会根据用户输入来搜索数据存储。 然后将用户问题与匹配结果结合起来，并使用提示（对 AI 或机器学习模型的明确指令）将其发送到 LLM，以生成所需的答案。







# 大模型应用层框架

## LangChain
LangChain 是一个框架，用于开发由 LLM 驱动的应用程序。

组件包
* PromptTemplate
* OutputParser 用于让 LLM 结构化输出并进行结果解析，方便后续的调用








# Reference
[大型语言模型（LLM）技术精要](https://zhuanlan.zhihu.com/p/597586623)
[Numbers every LLM Developer should know](https://github.com/ray-project/llm-numbers)