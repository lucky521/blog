---
title: "Flink部署和开发实践"
categories: [framework]
layout: post
---

# 流式数据处理概念

## 基本概念

本地状态，存储中间信息、缓存信息。

窗口操作

事件驱动

管道 允许输入流数据，输出流数据，交给下一个任务

DAG

## 分层API

最高层 sql api
data stream api
process function api


## 批处理和流处理都支持

batch application
stream application


# Flink功能

## StreamExecutionEnvironment

每一个flink应用都需要一个执行环境， 对于流处理程序，使用的执行环境类是 StreamExecutionEnvironment.
当env.execute()方法执行之后，代码所设计的一张执行图就会被打包发送到Flink Master，进行任务拆解和并行化，分配到TaskManager执行。

## Operator算子 DataStream Transformations

filter 过滤器，对数据流中的每个元素进行过滤判断，判断为true的元素进入下一个数据流

map可以理解为映射，对每个元素进行一定的变换后，映射为另一个元素。

flatmap 可以理解为将元素摊平，每个元素可以变为0个、1个、或者多个元素。

keyby DataStream → KeyedStream	

key Agg



join 和 coGroup

> Join转换使用来自两个输入的匹配记录对调用JoinFunction,这两个输入具有相同的键字段值.此行为与相等的内部联接非常相似.
> CoGroup转换在具有相同键值字段的两个输入的所有记录上调用带有迭代器的CoGroupFunction.如果输入没有某个键值的记录,则传递空迭代器.除了别的以外,CoGroup转换可以用于内部和外部的相等连接.因此它比Join变换更通用.


## window

Window是无限数据流处理的核心，Window将一个无限的stream拆分成有限大小的”buckets”桶，我们可以在这些桶上做计算操作。

窗口化的Flink程序的一般结构如下，第一个代码段中是分组的流，而第二段是非分组的流。正如我们所见，唯一的区别是分组的stream调用keyBy(…)和window(…)，而非分组的stream中window()换成了windowAll(…)

- 滚动窗口合并 TumblingEventTimeWindows
- 滑动窗口合并 SlidingEventTimeWindows
- Session窗口合并 EventTimeSessionWindows
- 间隔关联合并

官网对window的介绍 https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/joining.html


## source

flink中的source作为整个stream中的入口，而sink作为整个stream的终点。

SourceFunction为所有flink中source的根接口，其定义了run()方法和cancel()方法。

- run方法的功能是核心功能，主要用于source往出emit元素
- cancel方法时用于取消run方法的执行，一般来说run方法内部是一个循环，cancel方法中控制run方法的循环不满足条件，从而取消run方法的执行。


addSource(sourceFunction)


## sink

Sink是流的重点，根接口是sinkFunction。

其重要的方法是invoke()方法，用以实现结果数据的处理逻辑

SinkFunction 是一个接口，类似于SourceFunction接口。SinkFunction中主要包含一个方法，那就是用于数据输出的invoke 方法,每条记录都会执行一次invoke方法，用于执行输出操作。


## Connector 

用于支持与其他组件数据连接的 source 和 sink。比如和kafka连接，比如和Hadoop连接，比如和RaddbitMQ连接。

其中最为常用的当属Flink kafka connector。

此外，Apache Bahir 项目中也提供了更多连接器。



## Flink DDL


## watermark

水印就是一个时间戳，可以给每个消息添加一个 允许一定延迟 的时间戳

watermark是用于处理乱序事件的，通常用watermark机制结合window来实现

- 窗口可以继续计算一定时间范围内延迟的消息
- 添加水印后，窗口会等 5 秒，再执行计算。若超过5秒，则舍弃。
- 窗口执行计算时间由 水印时间 来触发，当接收到消息的 watermark >= endtime ，触发计算





# 部署

## standalone mode

mac环境下: /usr/local/Cellar/apache-flink/1.9.1/libexec 

./libexec/bin/start-cluster.sh

./libexec/bin/stop-cluster.sh

## yarn mode


## kubernetes mode 






# Flink HelloWorld

## Commands

```shell
cd /usr/local/Cellar/apache-flink/1.9.1 && ./libexec/bin/start-cluster.sh

./bin/flink run -c com.aaa.worldcount xxx.jar --host localhost --port 7777

./bin/flink list --all

./bin/flink cancel job_id
```



## scala code

```java
import org.apache.flink.api.scala._

object FlinkWordCount {
  def main(args:Array[String]):Unit = {
    //val env = ExecutionEnvironment.getExecutionEnvironment;
    val env = ExecutionEnvironment.createRemoteEnvironment("flink-master", 6123, "D:\\CodeBase\\jvmlearning\\flink-learning\\target\\flink-learning-1.0-SNAPSHOT.jar")
    
	val text = env.readTextFile("hdfs://flink-master:9000/user/flink/input/SogouQ.sample")
    
	println(text.count())
    
	val counts = text.flatMap {  _.toLowerCase.split("\\W+") }
      .map { (_, 1) }
      .groupBy(0)
      .sum(1)
    
	//env.execute()
    println(counts.count())
    //println(env.getExecutionPlan());
    //counts.print()
  }
}
```



## java code

这是我在IDEA上编译运行的第一个flink程序（maven构建）。

可以通过在命令行 $ nc -lk 9000 来往flink程序里输入字节流。

```java
package myflink;
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.util.Collector;

public class SocketWindowWordCount {
    public static void main(String[] args) throws Exception {
        // Create the execution environment.
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // Get the input data by connecting the socket.
        // Here it is connected to the local port 9000.
        // If the port 9000 has been already occupied, change to another port.
        DataStream<String> text = env.socketTextStream("localhost", 9000, "\n");
        // Parse the data, and group, windowing and aggregate it by word.
        DataStream<Tuple2<String, Integer>> windowCounts = text
                .flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
                    @Override
                    public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
                        for (String word : value.split("\\s")) {
                            out.collect(Tuple2.of(word, 1));
                        }
                    }
                })
                .keyBy(0)
                .timeWindow(Time.seconds(10))
                .sum(1);
        // Print the results to the console, note that here it uses the single-threaded printing instead of multi-threading
        windowCounts.print().setParallelism(1);
        env.execute("Socket Window WordCount");
    }
}
```


```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // 添加自定义数据源
        DataStreamSource<Person> data = env.addSource(new MyMysqlSource());
        data.print().setParallelism(2);
        data.addSink(new MyMysqlSink());
        // 提交执行任务
env.execute("MySourceMysql");
```


## Flink Job

首先你得把java或scala程序变成jar包，直接使用mave的package功能（注意maven指定的jdk版本要和运行时的版本一致）。

打开 http://localhost:8081/#/overview ，在Web界面提交job。

然后在Task Manager里面就可以看到自己提交的job，日志和标准输出都可以看到。







# 参考

flink基本概念介绍 https://www.jianshu.com/p/2ee7134d7373

如何正确使用 flink connector https://yq.aliyun.com/articles/716838

idea+maven打jar包  https://blog.csdn.net/branwel/article/details/79918018

官网的内容超级全 https://ci.apache.org/projects/flink/flink-docs-release-1.10/