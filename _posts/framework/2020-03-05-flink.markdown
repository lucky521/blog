---
title: "Flink部署和开发实践"
categories: [framework]
layout: post
---

# 流式数据处理概念

## 基本概念

本地状态，存储中间信息、缓存信息。

窗口操作

事件驱动

管道 允许输入流数据，输出流数据，交给下一个任务

DAG

## 分层API

data stream api
process function api
最高层 sql api


## 批处理和流处理都支持

batch application
stream application





# Flink功能

## StreamExecutionEnvironment

每一个flink应用都需要一个执行环境， 对于流处理程序，使用的执行环境类是 StreamExecutionEnvironment.
当env.execute()方法执行之后，代码所设计的一张执行图就会被打包发送到Flink Master，进行任务拆解和并行化，分配到TaskManager执行。

## 几种stream

DataStream

SingleOutputStreamOperator


## Operator算子 DataStream Transformations

filter 过滤器，对数据流中的每个元素进行过滤判断，判断为true的元素进入下一个数据流

flatmap 可以理解为将元素摊平，每个元素可以变为0个、1个、或者多个元素。

map 可以理解为映射，对每个元素进行一定的变换后，映射为另一个元素。

name 方法Sets the name of the current data stream.

returns 方法Adds a type information hint about the return type of this operator.


keyby DataStream → KeyedStream	

key Agg



### join 和 coGroup

> Join转换使用来自两个输入的匹配记录对调用JoinFunction,这两个输入具有相同的键字段值.此行为与相等的内部联接非常相似.
> CoGroup转换在具有相同键值字段的两个输入的所有记录上调用带有迭代器的CoGroupFunction.如果输入没有某个键值的记录,则传递空迭代器.除了别的以外,CoGroup转换可以用于内部和外部的相等连接.因此它比Join变换更通用.


## window

Window是无限数据流处理的核心，Window将一个无限的stream拆分成有限大小的”buckets”桶，我们可以在这些桶上做计算操作。

分组的流 vs 非分组的流。唯一的区别是分组的stream调用keyBy(…)和window(…)，而非分组的stream中window()换成了windowAll(…)

- 滚动窗口合并 TumblingEventTimeWindows 
固定大小窗口，不重叠，一个贴一个，同一元素不会分配到多个窗口。
- 滑动窗口合并 SlidingEventTimeWindows
固定大小窗口，多了一个滑动参数，每次滑动都按滑动参数大小，窗口大小>滑动大小的话，就会有一部分重叠，落入重叠的元素会分配到多个窗口。
- Session窗口合并 EventTimeSessionWindows
不同流之间的窗口不是按时间分组的，而是按各自的session分组。session的划分是当固定时间周期内不受到元素，则窗口关闭。
- 间隔关联合并

官网对window的介绍 https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/joining.html

## Event Time

setStreamTimeCharacteristic

- TimeCharacteristic.EventTime 
事件产生的时间，即数据产生时自带时间戳
- TimeCharacteristic.ProcessingTime
与数据本身的时间戳无关，即在window窗口内计算完成的时间（默认的Time）
- TimeCharacteristic.IngestionTime
数据进入到Flink的时间


## watermark

watermark是和Event Time一起使用的一个概念。由于消息自身的时间和消息被flink处理的时间往往是不同的，为了准备的表达数据的处理进度，出现了水印的概念。

水印就是一个时间戳，可以给每个消息添加一个 允许一定延迟 的时间戳。

watermark是用于处理乱序事件的，通常用watermark机制结合window来实现

DataStream.assignTimestampsAndWatermarks()方法来提取事件时间并同时产生水印

- 窗口可以继续计算一定时间范围内延迟的消息
- 添加水印后，窗口会等 5 秒，再执行计算。若超过5秒，则舍弃。
- 窗口执行计算时间由 水印时间 来触发，当接收到消息的 watermark >= endtime ，触发计算




## source

flink中的source作为整个stream中的入口，而sink作为整个stream的终点。

SourceFunction为所有flink中source的根接口，其定义了run()方法和cancel()方法。

- run方法的功能是核心功能，主要用于source往出emit元素
- cancel方法时用于取消run方法的执行，一般来说run方法内部是一个循环，cancel方法中控制run方法的循环不满足条件，从而取消run方法的执行。


addSource(sourceFunction)


## sink

Sink是流的重点，根接口是sinkFunction。

其重要的方法是invoke()方法，用以实现结果数据的处理逻辑

SinkFunction 是一个接口，类似于SourceFunction接口。SinkFunction中主要包含一个方法，那就是用于数据输出的invoke 方法,每条记录都会执行一次invoke方法，用于执行输出操作。


## Connector 

用于支持与其他组件数据连接的 source 和 sink。比如和kafka连接，比如和Hadoop连接，比如和RaddbitMQ连接。

其中最为常用的当属Flink kafka connector。

此外，Apache Bahir 项目中也提供了更多连接器。

### FlinkKafkaConsumer
kafka 中数据都是以二进制 byte 形式存储的。读到 Flink 系统中之后，需要将二进制数据转化为具体的 java、scala 对象。所以需要实现一个 schema 类，定义如何序列化和反序列数据。

反序列化时需要实现 DeserializationSchema 接口，并重写 deserialize(byte[] message) 函数。
如果是反序列化 kafka 中 kv 的数据时，需要实现 KeyedDeserializationSchema 接口，并重写 deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset) 函数。

- DeserializationSchema 接口类
下面是三个内置的常用序列化类
  * SimpleStringSchema，按字符串方式进行序列化、反序列化。
  * TypeInformationSerializationSchema，它可根据 Flink 的 TypeInformation 信息来推断出需要选择的 schema。
  * JsonDeserializationSchema 使用 jackson 反序列化 json 格式消息，并返回 ObjectNode，可以使用 .get(“property”) 方法来访问相应字段。



## Flink DDL





# Flink部署

## standalone mode

mac环境下: 用brew安装flink， $ brew install apache-flink
* brew安装的flink会放置在 /usr/local/Cellar/apache-flink/1.9.1/libexec 

* ./libexec/bin/start-cluster.sh

* ./libexec/bin/stop-cluster.sh

## yarn mode


## kubernetes mode 






# Flink HelloWorld

下面是一些最为简单的例子程序

## Basic Commands

```shell
cd /usr/local/Cellar/apache-flink/1.9.1 && ./libexec/bin/start-cluster.sh

./bin/flink run -c com.aaa.worldcount xxx.jar --host localhost --port 7777

./bin/flink list --all

./bin/flink cancel job_id
```



## scala code

```java
import org.apache.flink.api.scala._

object FlinkWordCount {
  def main(args:Array[String]):Unit = {
    //val env = ExecutionEnvironment.getExecutionEnvironment;
    val env = ExecutionEnvironment.createRemoteEnvironment("flink-master", 6123, "D:\\CodeBase\\jvmlearning\\flink-learning\\target\\flink-learning-1.0-SNAPSHOT.jar")
    
	val text = env.readTextFile("hdfs://flink-master:9000/user/flink/input/SogouQ.sample")
    
	println(text.count())
    
	val counts = text.flatMap {  _.toLowerCase.split("\\W+") }
      .map { (_, 1) }
      .groupBy(0)
      .sum(1)
    
	//env.execute()
    println(counts.count())
    //println(env.getExecutionPlan());
    //counts.print()
  }
}
```



## java code

这是我在IDEA上编译运行的第一个flink程序（maven构建）。

可以通过在命令行 $ nc -lk 9000 来往flink程序里输入字节流。

```java
package myflink;
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.util.Collector;

public class SocketWindowWordCount {
    public static void main(String[] args) throws Exception {
        // Create the execution environment.
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // Get the input data by connecting the socket.
        // Here it is connected to the local port 9000.
        // If the port 9000 has been already occupied, change to another port.
        DataStream<String> text = env.socketTextStream("localhost", 9000, "\n");
        // Parse the data, and group, windowing and aggregate it by word.
        DataStream<Tuple2<String, Integer>> windowCounts = text
                .flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
                    @Override
                    public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
                        for (String word : value.split("\\s")) {
                            out.collect(Tuple2.of(word, 1));
                        }
                    }
                })
                .keyBy(0)
                .timeWindow(Time.seconds(10))
                .sum(1);
        // Print the results to the console, note that here it uses the single-threaded printing instead of multi-threading
        windowCounts.print().setParallelism(1);
        env.execute("Socket Window WordCount");
    }
}
```


```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // 添加自定义数据源
        DataStreamSource<Person> data = env.addSource(new MyMysqlSource());
        data.print().setParallelism(2);
        data.addSink(new MyMysqlSink());
        // 提交执行任务
env.execute("MySourceMysql");
```


## Flink Job

首先你得把java或scala程序变成jar包，直接使用mave的package功能（注意maven指定的jdk版本要和运行时的版本一致）。

打开 http://localhost:8081/#/overview ，在Web界面提交job。

然后在Task Manager里面就可以看到自己提交的job，其日志和标准输出都可以看到。







# 参考

flink基本概念介绍 https://www.jianshu.com/p/2ee7134d7373

如何正确使用 flink connector https://yq.aliyun.com/articles/716838

idea+maven打jar包  https://blog.csdn.net/branwel/article/details/79918018

官网的内容超级全 https://ci.apache.org/projects/flink/flink-docs-release-1.10/